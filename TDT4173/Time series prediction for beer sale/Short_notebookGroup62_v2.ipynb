{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Project report for Group 62 (short notebook)\n",
    "\n",
    "**We did the Kaggle competition named Sales Forecasting and are named Group 62 as Kaggle Group name.**\n",
    "\n",
    "The names and student IDs for those on Group 62 are:\n",
    "1. **Ruben Mustad, 480858**\n",
    "2. **Aneeq Ahsan, 546865**\n",
    "3. **Axel Luiggi-Gørrissen, 554086**\n",
    "\n",
    "This notebook contains all the necessary information to create our two chosen submissions on Kaggle. Only the given dataset was used, and features was created using the information found in these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.0\n",
      "1.3.4\n",
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # version 1.21.0 was used\n",
    "import pandas as pd # version 1.3.4 was used\n",
    "import lightgbm as lgbm # version 3.2.1 was used\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(lgbm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag(train, test):\n",
    "    # Create lag features for our model\n",
    "    # We use lag of 60, 90, 180 and 365\n",
    "    train['train'] = 1\n",
    "    test['train'] = 0\n",
    "    combined = pd.concat([train,test])\n",
    "    lag = [60, 90, 180, 365]\n",
    "    lag_cols = [f'lag_{lag}' for lag in lag]\n",
    "    lag_colP = [f'lagPromo_{lag}' for lag in lag]\n",
    "    lagPromo = [f'lag{lag}Promo' for lag in lag]\n",
    "    for lags, lag_col in zip(lag,lag_cols): # create lag features\n",
    "        combined[lag_col] = combined[['ts_id', 'Sales']].groupby('ts_id')['Sales'].shift(lags)\n",
    "    for lags, lag_colPs in zip(lag,lag_colP): \n",
    "        combined[lag_colPs] = combined[['ts_id', 'isPromo']].groupby('ts_id')['isPromo'].shift(lags)\n",
    "    train = combined[combined['train'] == 1]\n",
    "    test = combined[combined['train'] == 0]\n",
    "    train.drop(['train'], axis = 1, inplace = True)\n",
    "    test.drop(['train'], axis = 1, inplace = True)\n",
    "    def lag_creator(df): # create features with information about the lagged sale number and if it was promo or not\n",
    "        df['lag60Promo'] = df['lag_60'].astype(str) + df['lagPromo_60'].astype(str)\n",
    "        df['lag180Promo'] = df['lag_180'].astype(str) + df['lagPromo_180'].astype(str)\n",
    "        df['lag90Promo'] = df['lag_90'].astype(str) + df['lagPromo_90'].astype(str)\n",
    "        df['lag365Promo'] = df['lag_365'].astype(str) + df['lagPromo_365'].astype(str)\n",
    "        return df\n",
    "    train = lag_creator(train)\n",
    "    test = lag_creator(test)\n",
    "    remove = ['lagPromo_60', 'lagPromo_90', 'lagPromo_180', 'lagPromo_365']\n",
    "    for i in remove:\n",
    "        train.pop(i)\n",
    "        test.pop(i)\n",
    "    del(train['id'])\n",
    "    del(test['Sales'])\n",
    "    return train, test\n",
    "def merge(df, sku_features, id_map):\n",
    "    # Merge the given datasets\n",
    "    df = pd.merge(df, id_map, how='left', on='ts_id')\n",
    "    df = pd.merge(df, sku_features, how='left', on='SKU')\n",
    "    return df\n",
    "def time_features(df):\n",
    "    # Add time features to our model\n",
    "    df['day_of_month'] = df['Date'].dt.day\n",
    "    df['day_of_week'] = df['Date'].dt.weekday\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    df['week'] = df['Date'].dt.week\n",
    "    return df\n",
    "def promo_features(df):\n",
    "    # Add promo features to most of the features in the given dataset\n",
    "    df['ts_promo'] = df['ts_id'].astype(str) + df['isPromo'].astype(str)\n",
    "    df['store_promo'] = df['Store'].astype(str) + df['isPromo'].astype(str)\n",
    "    df['segment_promo'] = df['Segment'].astype(str) + df['isPromo'].astype(str)\n",
    "    df['brand_promo'] = df['Brand'].astype(str) + df['isPromo'].astype(str)\n",
    "    df['sku_promo'] = df['SKU'].astype(str) + df['isPromo'].astype(str)\n",
    "    df['dom_promo'] = df['day_of_month'].astype(str) + df['isPromo'].astype(str)\n",
    "    df['dow_promo'] = df['day_of_week'].astype(str) + df['isPromo'].astype(str)\n",
    "    return df\n",
    "def brand_fixer(df):\n",
    "    # Fix the mistake in Baltik N0\n",
    "    SKU_baltik0 = df['SKU'] == '_Baltika №0 Can 0,45'\n",
    "    Brand_baltik0 = df['Brand'] == 'Baltika №3'\n",
    "    SKU_index = SKU_baltik0[SKU_baltik0].index\n",
    "    Brand_index = Brand_baltik0[Brand_baltik0].index\n",
    "    \n",
    "    baltik0 = list(set(SKU_index).intersection(Brand_index))\n",
    "    df['Brand'][baltik0] = 'Baltika №0'\n",
    "    return df\n",
    "def lager_fix(train, test):\n",
    "    # We found information about Firmennoe and Baltika Draught been lager\n",
    "    for i in range(len(train)):\n",
    "        if train['SKU'][i] == '_Firmennoe PET 1,42':\n",
    "            train['Product'][i] = 'lager'\n",
    "        elif train['SKU'][i] == '_Baltika Draught PET 0,47':\n",
    "            train['Product'][i] = 'lager'\n",
    "    for i in range(len(test)):\n",
    "        if test['SKU'][i] == '_Firmennoe PET 1,42':\n",
    "            test['Product'][i] = 'lager'\n",
    "        elif test['SKU'][i] == '_Baltika Draught PET 0,47':\n",
    "            test['Product'][i] = 'lager'\n",
    "    return train, test\n",
    "def remove2015(train):\n",
    "    # Remove all of 2015\n",
    "    train = train[train['year'] > 2015]\n",
    "    return train\n",
    "\n",
    "def remove_values(train, upper, below):\n",
    "    print('Remove sales values above ' + str(upper) + ' and below ' + str(below))\n",
    "    train = train[train['Sales'] < upper]  # remove very large sales values\n",
    "    train = train[train['Sales'] >= below]  # remove negative large sales values\n",
    "    return train\n",
    "def interesting_days(df):\n",
    "    # Since most of people in Russia are Orthodoxian, they do not celebrate christmas before start of january, but some study in 2015 \n",
    "    # shows that Russia is starting to follow a more western trend when it comes to shopping (e.g., they shop a lot in mid-late december)\n",
    "    # Therefore we chooses week 51-53. \n",
    "    df['special_days'] = df['week'].between(51, 53).astype(int) # christmas\n",
    "    # The summer period (as we understood it) is also longer, but based on weather and when people usually have their vacation and stuff\n",
    "    # we narrowed it down to week 26-31\n",
    "    df['special_days'] += df['week'].between(26, 31).astype(int) # summer\n",
    "    df['special_days'] += (df['week'] == 44).astype(int)\n",
    "    df['special_daysPromo'] = df['special_days'].astype(str) + df['isPromo'].astype(str)\n",
    "\n",
    "    # There seems to be more sale on friday and saturday, so they might bring some insight.\n",
    "    df['party_time'] = df['day_of_week'].between(4,5).astype(int)\n",
    "    df['party_timePromo'] = df['party_time'].astype(str) + df['isPromo'].astype(str)\n",
    "    \n",
    "    # School holidays in Russia \n",
    "    df['School_holiday'] = df['Date'].between('2015-01-01', '2015-01-15').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2015-03-23', '2015-03-27').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2015-05-26', '2015-08-31').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2015-11-02', '2015-11-06').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2016-01-01', '2016-01-15').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2016-03-21', '2016-03-25').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2016-05-30', '2016-08-31').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2016-11-07', '2016-11-11').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2017-01-02', '2017-01-15').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2017-03-22', '2017-03-29').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2017-06-01', '2017-09-01').astype(int)\n",
    "    df['School_holiday'] += df['Date'].between('2017-11-05', '2017-11-12').astype(int)\n",
    "    df['School_holidayPromo'] = df['School_holiday'].astype(str) + df['isPromo'].astype(str)\n",
    "    return df\n",
    "def promo_relation(df):\n",
    "    # Add information about how long time ago there was promo and how long until next\n",
    "    def days_to_promo(df):\n",
    "        df[\"t\"] = (df[\"Date\"].where(df[\"isPromo\"] > 0.0).bfill() - df[\"Date\"]).dt.days\n",
    "        return df\n",
    "    def days_since_promo(df):\n",
    "        df[\"t\"] =  (df[\"Date\"] - df[\"Date\"].where(df[\"isPromo\"] > 0.0).ffill()).dt.days\n",
    "        return df\n",
    "    \n",
    "    df[\"days_to_promo\"] = df.groupby([df.ts_id])[[\"Date\", \"isPromo\", \"SKU\", \"Store\"]].apply(days_to_promo)[\"t\"]\n",
    "    df[\"days_since_promo\"] = df.groupby([df.ts_id])[[\"Date\", \"isPromo\", \"SKU\", \"Store\"]].apply(days_since_promo)[\"t\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_dataset(model3 = False, model4 = False):\n",
    "    # Since model1, 3 and 4 are quite similar, we can use a single function to construct the dataset\n",
    "    train = pd.read_csv('data/beer_train.csv',parse_dates=['Date'])\n",
    "    test = pd.read_csv('data/beer_test.csv',parse_dates=['Date'])\n",
    "    sku_features = pd.read_csv('data/sku_features.csv')\n",
    "    id_map = pd.read_csv('data/id_store_sku.csv')\n",
    "    \n",
    "    train, test = create_lag(train, test)\n",
    "    train = merge(train, sku_features, id_map)\n",
    "    test = merge(test, sku_features, id_map)\n",
    "    train = time_features(train)\n",
    "    test = time_features(test)\n",
    "    train = promo_features(train)\n",
    "    test = promo_features(test)\n",
    "    if model3 or model4:\n",
    "        train = promo_relation(train)\n",
    "        test = promo_relation(test)\n",
    "        \n",
    "    train = brand_fixer(train)\n",
    "    test = brand_fixer(test)\n",
    "    train, test = lager_fix(train, test)\n",
    "    train = remove2015(train)\n",
    "    \n",
    "    if model4:\n",
    "        remove = ['lag60Promo', 'lag180Promo', 'lag90Promo', 'lag365Promo', 'Brand']\n",
    "        for i in remove:\n",
    "            train.pop(i)\n",
    "            test.pop(i)\n",
    "\n",
    "    train = train.replace(to_replace='nannan', value=np.nan)\n",
    "    test = test.replace(to_replace='nannan', value=np.nan)\n",
    "    return train, test\n",
    "\n",
    "def model2_dataset():\n",
    "    train = pd.read_csv('data/beer_train.csv',parse_dates=['Date'])\n",
    "    test = pd.read_csv('data/beer_test.csv',parse_dates=['Date'])\n",
    "    sku_features = pd.read_csv('data/sku_features.csv')\n",
    "    id_map = pd.read_csv('data/id_store_sku.csv')\n",
    "    \n",
    "    train, test = create_lag(train, test)\n",
    "    train = merge(train, sku_features, id_map)\n",
    "    test = merge(test, sku_features, id_map)\n",
    "    train = time_features(train)\n",
    "    test = time_features(test)\n",
    "    train = promo_features(train)\n",
    "    test = promo_features(test)\n",
    "\n",
    "    train = interesting_days(train)\n",
    "    test = interesting_days(test)\n",
    "    train = remove_values(train, 15, 0)\n",
    "    return train, test\n",
    "\n",
    "def prepare_dataset(train, test):\n",
    "    train_y = train['Sales']\n",
    "    del(train['Sales'], train['Date'])\n",
    "    train_features = train\n",
    "    del(test['Date'], test['id'])\n",
    "    test_features = test\n",
    "    for c in test_features.columns:\n",
    "        test_features[c] = test_features[c].astype('category')\n",
    "        train_features[c] = train_features[c].astype('category')\n",
    "    print('Number of features in train set:', len(train_features.columns))\n",
    "    print('Number of features in test set:', len(test_features.columns))\n",
    "    print('Features used in model:', train.columns)\n",
    "    return train_y, train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in train set: 29\n",
      "Number of features in test set: 29\n",
      "Features used in model: Index(['ts_id', 'isPromo', 'lag_60', 'lag_90', 'lag_180', 'lag_365',\n",
      "       'lag60Promo', 'lag180Promo', 'lag90Promo', 'lag365Promo', 'Store',\n",
      "       'SKU', 'Segment', 'Pack', 'Product', 'Brand', 'Volume', 'day_of_month',\n",
      "       'day_of_week', 'month', 'year', 'week', 'ts_promo', 'store_promo',\n",
      "       'segment_promo', 'brand_promo', 'sku_promo', 'dom_promo', 'dow_promo'],\n",
      "      dtype='object')\n",
      "Remove sales values above 15 and below 0\n",
      "Number of features in train set: 35\n",
      "Number of features in test set: 35\n",
      "Features used in model: Index(['ts_id', 'isPromo', 'lag_60', 'lag_90', 'lag_180', 'lag_365',\n",
      "       'lag60Promo', 'lag180Promo', 'lag90Promo', 'lag365Promo', 'Store',\n",
      "       'SKU', 'Segment', 'Pack', 'Product', 'Brand', 'Volume', 'day_of_month',\n",
      "       'day_of_week', 'month', 'year', 'week', 'ts_promo', 'store_promo',\n",
      "       'segment_promo', 'brand_promo', 'sku_promo', 'dom_promo', 'dow_promo',\n",
      "       'special_days', 'special_daysPromo', 'party_time', 'party_timePromo',\n",
      "       'School_holiday', 'School_holidayPromo'],\n",
      "      dtype='object')\n",
      "Number of features in train set: 31\n",
      "Number of features in test set: 31\n",
      "Features used in model: Index(['ts_id', 'isPromo', 'lag_60', 'lag_90', 'lag_180', 'lag_365',\n",
      "       'lag60Promo', 'lag180Promo', 'lag90Promo', 'lag365Promo', 'Store',\n",
      "       'SKU', 'Segment', 'Pack', 'Product', 'Brand', 'Volume', 'day_of_month',\n",
      "       'day_of_week', 'month', 'year', 'week', 'ts_promo', 'store_promo',\n",
      "       'segment_promo', 'brand_promo', 'sku_promo', 'dom_promo', 'dow_promo',\n",
      "       'days_to_promo', 'days_since_promo'],\n",
      "      dtype='object')\n",
      "Number of features in train set: 26\n",
      "Number of features in test set: 26\n",
      "Features used in model: Index(['ts_id', 'isPromo', 'lag_60', 'lag_90', 'lag_180', 'lag_365', 'Store',\n",
      "       'SKU', 'Segment', 'Pack', 'Product', 'Volume', 'day_of_month',\n",
      "       'day_of_week', 'month', 'year', 'week', 'ts_promo', 'store_promo',\n",
      "       'segment_promo', 'brand_promo', 'sku_promo', 'dom_promo', 'dow_promo',\n",
      "       'days_to_promo', 'days_since_promo'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_model1, test_model1 = model1_dataset()\n",
    "train_y1, train_features1, test_features1 = prepare_dataset(train_model1, test_model1)\n",
    "\n",
    "train_model2, test_model2 = model2_dataset()\n",
    "train_y2, train_features2, test_features2 = prepare_dataset(train_model2, test_model2)\n",
    "\n",
    "# Model3 scores best\n",
    "train_model3, test_model3 = model1_dataset(model3 = True)\n",
    "train_y3, train_features3, test_features3 = prepare_dataset(train_model3, test_model3)\n",
    "\n",
    "# Model4: Here we remove the lag promos and brand (as we got bad feature importance on them with RFE, RFECV, Shap)\n",
    "train_model4, test_model4 = model1_dataset(model4 = True)\n",
    "train_y4, train_features4, test_features4 = prepare_dataset(train_model4, test_model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission 1\n",
    "\n",
    "This will produce the delivery file on Kaggle called FinalSubmission2.csv, which scored 0.45878 on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 = pd.read_csv('data/intro_submission.csv')\n",
    "\n",
    "lgb_param_model1 = {'n_estimators': 1566, 'learning_rate': 0.004370719636735594, 'num_leaves': 122,  \n",
    "             'reg_alpha': 0, 'reg_lambda': 26, 'subsample': 0.9063104808260567, 'cat_l2': 17, 'cat_smooth': 10,\n",
    "             'colsample_bytree': 0.22250521336587764, 'max_cat_threshold': 55, 'min_data_per_group': 70}\n",
    "\n",
    "model_lgbm1 = lgbm.LGBMRegressor(**lgb_param_model1, random_state = 1)\n",
    "model_lgbm1.fit(train_features1, train_y1)\n",
    "prediction_model1 = model_lgbm1.predict(test_features1)\n",
    "\n",
    "lgb_param_model2 = {'colsample_bytree': 0.1595013602757808, 'learning_rate': 0.004230626203615872, 'max_depth': 8, \n",
    "             'n_estimators': 1833, 'num_leaves': 118, 'reg_alpha': 15.418374764903756, \n",
    "             'reg_lambda': 32.11982305383127, 'subsample': 0.8928231814360663}\n",
    "\n",
    "model_lgbm2 = lgbm.LGBMRegressor(**lgb_param_model2, random_state = 1)\n",
    "model_lgbm2.fit(train_features2, train_y2)\n",
    "prediction_model2 = model_lgbm2.predict(test_features2)\n",
    "\n",
    "submission1['Sales'] = 0.65*prediction_model1 + 0.35*prediction_model2\n",
    "submission1.to_csv('FinalSubmission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission 2\n",
    "\n",
    "This will produce the delivery file on Kaggle called FinalSubmission3.csv, which scored 0.45633 on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2 = pd.read_csv('data/intro_submission.csv')\n",
    "\n",
    "lgb_param_model3 = {'n_estimators': 1915, 'learning_rate': 0.0049319743191803544, 'num_leaves': 116, \n",
    "                    'reg_alpha': 34, 'reg_lambda': 49, 'subsample': 0.9872621712765213, 'subsample_freq': 1, \n",
    "                    'colsample_bytree': 0.11944549125971712, 'max_cat_threshold': 60, 'min_data_per_group': 93, \n",
    "                    'cat_l2': 22, 'cat_smooth': 12}\n",
    "\n",
    "submission3 = pd.read_csv('data/intro_submission.csv')\n",
    "model_lgbm3 = lgbm.LGBMRegressor(**lgb_param_model3, random_state = 1)\n",
    "model_lgbm3.fit(train_features3, train_y3)\n",
    "prediction_model3 = model_lgbm3.predict(test_features3)\n",
    "\n",
    "lgb_param_model4 = {'n_estimators': 2156, 'learning_rate': 0.004954170518408145, 'num_leaves': 108, \n",
    "             'reg_alpha': 40, 'reg_lambda': 21, 'subsample': 0.9261717756336443, 'subsample_freq': 1, \n",
    "             'colsample_bytree': 0.16903417694268677, 'max_cat_threshold': 63, 'min_data_per_group': 55, 'cat_l2': 23, 'cat_smooth': 15}\n",
    "model_lgbm1 = lgbm.LGBMRegressor(**lgb_param_model4, random_state = 1)\n",
    "model_lgbm1.fit(train_features4, train_y4)\n",
    "prediction_model4 = model_lgbm1.predict(test_features4)\n",
    "\n",
    "# Since model3 scores better, we put some more weight on it\n",
    "submission2['Sales'] = 0.60*prediction_model3 + 0.40*prediction_model4\n",
    "submission2.to_csv('FinalSubmission3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
