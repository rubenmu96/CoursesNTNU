---
title: 'Compulsory exercise 2: Group 38'
author: Lucas Michael Cammann, Ruben Mustad and Michinori Asaka
  Sciences, NTNU
header-includes:
 - \usepackage{amsmath}
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
subtitle: TMA4268 Statistical Learning V2022
urlcolor: blue
---




```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")
```


```{r, eval=T, echo = F}
library(MASS)
library(leaps)
library(glmnet)
library(Matrix)
library(mgcv)
library(splines)
library(gam)
library(tidyverse)
library(formatR)
library(ggpubr)
library(pls)
library(tidyverse)
library(palmerpenguins)
library(caret)
library(e1071)
library(lattice)
library(ggfortify)
library(ggplot2)
library(GGally)
library(randomForest)
library(palmerpenguins) # Contains the data set "penguins".
library(tree)
```


# Problem 1
## a) 
The Boston house dataset consists of 13 predictors and one response variable. We now want to use forward and backward stepwise selection to see how the predictors are going to affect our model.

```{r, eval=T, fig.width=10, fig.height=5}
par(mfrow=c(1,2))
set.seed(1)
boston <- scale(Boston, center = T, scale = T)
train.ind = sample(1:nrow(boston), 0.8 * nrow(boston))
boston.train = data.frame(boston[train.ind, ])
boston.test = data.frame(boston[-train.ind, ])

# Forward
forward <- regsubsets(medv~.,data = boston.train, 
                      nbest = 1, nvmax = ncol(boston) - 1, method=c('forward'))
forward_results = summary(forward)

plot(forward_results$adjr2, xlab='Number of Variables', ylab='Adj. R2',
     type='l', main = 'Forward')
plot(forward, scale = 'adjr2', main = 'Forward')

# Backward
backward <- regsubsets(medv~.,data = boston.train, 
                       nbest = 1, nvmax = ncol(boston) - 1, method=c('backward'))
backward_results = summary(backward)

plot(backward_results$adjr2, xlab='Number of Variables', ylab='Adj. R2', 
     type='l', main = 'Backward')
plot(backward, scale = 'adjr2', main = 'Backward')

```

From the figure above, we observe that both the forward and backward stepwise selections are quite similar. The best model (according to adjusted $R^2$) will be the model with every predictor in the base model except indus and age. However, we see that every predictor is included in the 3rd best model, which has only a slightly worse adjusted $R^2$ value. 


## b)
From the forward step-wise selection, we find that the four best predictors are
\begin{itemize}
\item rm: average number of rooms per dwelling
\item dis: Weighted distances to five Boston employment centers
\item ptratio: Pupil-teacher ratio by town
\item lstat: Percentage of lower status of the population
\end{itemize}
which gives us an adjusted $R^2$ score of 0.69.

 

## c) 

```{r, eval=T, fig.width=6, fig.height=5}
set.seed(1)

# Divide the training and test set into X (predictors) and y (response)
# for train and test
X_train = as.matrix(boston.train[,1:13])
y_train = as.matrix(boston.train[,14])
X_test = as.matrix(boston.test[,1:13])
y_test = as.matrix(boston.test[,14])


# Ridge: alpha = 0, Lasso: alpha = 1
lasso_cv = cv.glmnet(X_train, y_train, alpha = 1, nfold = 5)
plot(lasso_cv)

# Find the lambda that gives us the smallest MSE
lam = lasso_cv$lambda
best_lambda = lasso_cv$lambda.min
sprintf('The best lambda value is %#.5f', best_lambda)

# Train Lasso on the training set with the smallest lambda
lasso_4c = glmnet(X_train, y_train, alpha = 1, lambda = best_lambda)

# Make predictions on the training set and test set
lasso_train = predict(lasso_4c, X_train)
sprintf('The MSE on the training set is %#.4f', mean((lasso_train - y_train)^2))

lasso_test = predict(lasso_4c, X_test)
sprintf('The MSE on the test set is %#.4f', mean((lasso_test - y_test)^2))

```
The best $\lambda$ value (i.e., the one that gave us the smallest MSE) found using a 5-fold CV is $0.00217$. From the plot, we observe that the MSE decreases as the $\lambda$ (or $\log(\lambda)$) decreases until around $-5$, after this, the change in MSE is quite small. 

The MSE on the test set is 0.2045, while the MSE on the training set is 0.2757. It is a bit unusual to get better performance on the test set than on the training set, as we trained on the training set, but we will get back to this in Problem 3. 
 
 
Using a seed of 1, we see that Lasso gives us no zero coefficients. If we did not use a seed, we observed that the predictor indus had zero as a coefficient sometimes. Indus is also the coefficient with the lowest value when using a seed of 1.
```{r, eval=T, fig.width=6, fig.height=5}
coef(lasso_4c) # get the coefficients
```

## d)

\begin{enumerate}
\item True
\item False
\item False
\item True
\end{enumerate}


# Problem 2 (6P)

```{r, echo=FALSE}
set.seed(1)

# load a synthetic dataset
id <- "1CWZYfrLOrFdrIZ6Hv73e3xxt0SFgU4Ph" # google file ID
synthetic <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

# split into training and test sets
train.ind = sample(1:nrow(synthetic), 0.8 * nrow(synthetic)) 
synthetic.train = data.frame(synthetic[train.ind, ])
synthetic.test = data.frame(synthetic[-train.ind, ])

# show head(..)
# Y: response variable; X: predictor variable
#head(synthetic)
```

## a) (2P)
Fit PCR:
```{r, eval=TRUE}
pcr_model <- pcr(Y~., data = synthetic.train, scale = TRUE, validation = "CV")
```

Here is a graph of MSEP for PCR model:
```{r, echo=TRUE}
validationplot(pcr_model, val.type = "MSEP", type = "b", lwd = 2)
```

Fit PLSR:
```{r, eval=TRUE}
pls_model <- plsr(Y~., data = synthetic.train, scale = TRUE, validation = "CV")
```

Here is a graph of MSEP for PLSR model:
```{r, echo=TRUE}
validationplot(pls_model, val.type = "MSEP", type = "b", lwd = 2)
```


## b) (4P)

MSEP of PCR shows gradual decrease as a function of number of components, while that of PLS shows sharp decrease at a number of components of 1 and is quickly stabilized at a number of component of 4.

This is associated with characteristics of PLR and PCR. In PCR, the dimensionality reduction is done via an unsupervised method (PDA); It is therefore not garanteed that the directions that best explain the predictors will also be the best directions to use for predicting the response.
On the other hand, PLS puts highest weight on the variables that are most strongly related to the response when obtaining the first PLS direction.

Large MSEP reduction at a number of component of 1 is therefore expected if one of the predictor variables has strong correlation with the response. This is the case here, since a predictor variable X1 has strong correlation with the response:
```{r, echo=FALSE, fig.width=10, fig.height=10}
ggpairs(synthetic.train)+
  theme_minimal()
```

 

# Problem 3

## a)
\begin{enumerate}
\item True
\item False
\item True
\item True
\end{enumerate}


## b)

We will now fit an additive model on the Boston training set with the predictors rm, ptratio and lstat. We will let ptratio be a smoothing spline with 3 degrees of freedom and lstat be a 2nd-degree polynomial. 

```{r, eval=T}

model_task3 = gam(medv ~ rm + s(ptratio, df = 3) + poly(lstat, 2), data = boston.train)
# summary(model_task3)


gam_train = predict(model_task3, boston.train)
sprintf('The MSE on the training set is %#.4f', mean((gam_train - y_train)^2))

gam_test = predict(model_task3, boston.test)
sprintf('The MSE on the test set is %#.4f', mean((gam_test - y_test)^2))

```
Below we plot the polynomials and the spline.

```{r, eval=T, fig.width=10, fig.height=5}

par(mfrow=c(1,3))
plot(model_task3)
```

We again observe that we get better MSE on the test set than on the training set again. To understand why, it can be smart to do some plotting, so we decided to plot the observed values vs predicted values. We see that in the training set, there are quite a few observed values that have a value of $3$, that our model does not manage to predict well. In the test set, we don't see those, causing us to get a better MSE on the test set.

```{r, eval=T, fig.width=10, fig.height=5}

data_mod <- data.frame(Predicted = predict(model_task3, boston.train),
                       Observed = boston.train$medv)

data_mod_test <- data.frame(Predicted = predict(model_task3, boston.test),
                       Observed = boston.test$medv)

plot1 <- ggplot(data_mod,
       aes(x = Predicted, y = Observed)) +
ggtitle('Training set') +
geom_point() +
geom_abline(intercept = 0,
            slope = 1,
            color = "red",
            size = 2)

plot2 <- ggplot(data_mod_test,
       aes(x = Predicted, y = Observed)) +
ggtitle('Test set') + 
geom_point() +
geom_abline(intercept = 0,
            slope = 1,
            color = "red",
            size = 2)

ggarrange(plot1, plot2, 
          ncol = 2, nrow = 1)
```

# Problem 4 (11P)

## a) (2P) - Multiple choice
1: False\
2: True\
3: True\
4: True\

## b) (2P)

```{r, echo=FALSE, out.width="60%"}
library(imager)
knitr::include_graphics("C:/Users/nebur/Downloads/Problem_4_b.jpg")

```


## c) (4P)
  
```{r, echo=FALSE}
data(penguins)

names(penguins) <- c("species","island","billL","billD","flipperL","mass","sex","year")

Penguins_reduced <- penguins %>%  
  dplyr::mutate(mass = as.numeric(mass),  
                flipperL = as.numeric(flipperL),
                year = as.numeric(year)) %>% 
  drop_na()

# We do not want "year" in the data (this will not help for future predictions)
Penguins_reduced <- Penguins_reduced[,-c(8)]

set.seed(4268)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

1)\
Generate a simple classification tree using the Gini index:
```{r, eval=TRUE}
tree.Penguins = tree(species~., data = train, split = "gini")
```

Plot of the resulting tree:
```{r, echo=TRUE, fig.width=6, fig.height=5}
plot(tree.Penguins, type = "uniform")
text(tree.Penguins, pretty = 1)
```

2)\
Apply cost-complexity pruning using 10-fold CV:
```{r, eval=TRUE}
set.seed(123)
tree.Penguins <- tree(species~., data = train)
cv.Penguins <- cv.tree(tree.Penguins, K = 10)
```

Plot the deviance as a function of tree size:
```{r, echo=TRUE, , fig.width=4, fig.height=4}
plot(cv.Penguins$dev ~ cv.Penguins$size, type = "b", lwd = 2, col = "red", xlab = "Terminal nodes", ylab = "Deviance")
```
This plot suggests that 4 leaves would work well.

3)\
Run cross-validation and plot the number of misclassifications as a function of tree size:
```{r, eval=TRUE, fig.width=4, fig.height=4}
set.seed(123)
tree.Penguins <- tree(species~., data = train)
cv.Penguins = cv.tree(tree.Penguins, FUN = prune.misclass)
plot(cv.Penguins$dev~ cv.Penguins$size, type = "b", lwd = 2, col = "red",  xlab = "Terminal nodes", ylab = "Misclassifications")
```

This plot suggests that 3-8 nodes gives similarly low misclassifications.\
So we choose 3 terminal nodes, which is the smallest model (lowest variance), as an optimal model parameter.\

Perform prediction on the test set with 3 nodes and calculate the misclassification rate:
```{r, eval=TRUE}
bestmod.Penguins <- prune.tree(tree.Penguins, best = 3)
tree.pred = predict(bestmod.Penguins, newdata = test, type = "class") 
misclass.tree = table(tree.pred, test$species) 
MER <- (nrow(test)-sum(diag(misclass.tree))) / nrow(test)
```

The misclassification rate is:
```{r, echo=FALSE}
MER
```


## d) (3P)

Construct a classification tree based on random forests.\
We set mtry = 2, since it is approximately equal to sqrt(number of variables):
```{r, eval=TRUE}
rf.Penguins = randomForest(species~., data = train, mtry = 2, importance = TRUE)
```

Perform prediction on the test set and calculate the misclassification rate:
```{r, eval=TRUE}
rf.pred = predict(rf.Penguins, newdata = test)
misclass.rf = table(rf.pred, test$species)
MER <- (nrow(test)-sum(diag(misclass.rf))) / nrow(test)
```
The misclassification rate is:
```{r, echo=FALSE}
MER
```

The variable importance plots suggest that bill length and flipper length are the most influential parameters in the prediction of the species:
```{r, echo=FALSE, fig.width=6, fig.height=5}
varImpPlot(rf.Penguins, pch = 20, main = "")
```

# Problem 5 (6P) 
  
## a) (2P) - Multiple choice  
  
(i) Logistic regression is the preferred method for this data set, because it gives nice interpretable parameter estimates.
FALSE
(ii) In this dataset we are guaranteed to find a separating hyperplane, unless there are exact feature ties (two patients with the exact same gene data, but different outcome).
FALSE
(iii) When fitting a support vector classifier, we usually have to standardize the variables first. 
TRUE
(iv) By choosing a smaller budget parameter $C$ we are making the model less biased, but introduce more variance.
TRUE 



## b) (4P)

(i) (2P) 
```{r,eval=T}
# First for the linear boundary
linres <- tune.svm(species ~., data = train, kernel = "linear", cost = c(0.1,1,10))
linerror <- linres$best.performance
lincost <- linres$best.model$cost
# Then for the radial boundary
radres <- tune.svm(species ~., data = train, kernel = "radial", cost = c(0.1,1,10), gamma = c(10^-2, 10^-1,1))
raderror <- radres$best.performance
radparams <- radres$best.parameters
```
The best achieved training error rate for the support vector classifier is $`r linerror`$ with a cost parameter of $`r lincost``$ and for the support vector machine $`r raderror``$ with the parameter combination of $`r radparams``$. 

(ii) (1P) Report the confusion tables and misclassification error rates for the test set in both cases, using the best parameters you found in (i). 
```{r, eval = T, echo = T}
linpred <- predict(linres$best.model, test)
confmlin <- confusionMatrix(linpred,test$species)$table
missclin <- mean(linpred != test$species)
radpred <- predict(radres$best.model, test)
confmrad <- confusionMatrix(radpred,test$species)$table
misscrad <- mean(radpred != test$species)
```
The confusion matrix for the support vector classifier is $`r confmlin`$, and for the support vector machine is $`r confmrad`$. The misclassification rate is $`r missclin`$ in the former case and $`r misscrad`$ in the latter.


(iii) (1P) Which classifier do you prefer and why? 
In the current scenario, the support vector classifier (linear boundary) is to be preferred. The reason for this is twofold: Firstly, the support vector classifier is a simpler classifier with only one hyperparameter, which makes the tuning and interpretation easier for the analyst and the respective audience. Secondly, the support vector classifier has actually been shown to outperform the support vector machine on basis of the test misclassification error, as shown above. 



# Problem 6 (12P)

```{r}
# load a synthetic dataset
id <- "1NJ1SuUBebl5P8rMSIwm_n3S8a7K43yP4" # google file ID
happiness <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
                      fileEncoding="UTF-8-BOM")

colnames(happiness)
cols = c('Country.name', 
         'Ladder.score',  # happiness score
         'Logged.GDP.per.capita',  
         'Social.support', 
         'Healthy.life.expectancy', 
         'Freedom.to.make.life.choices',
         'Generosity',  # how generous people are
         'Perceptions.of.corruption')

# We continue with a subset of 8 columns:
happiness = subset(happiness, select = cols)
rownames(happiness) <- happiness[, c(1)]

# And we creat an X and a Y matrix
happiness.X = happiness[, -c(1, 2)]
happiness.Y = happiness[, c(1, 2)]
happiness.XY = happiness[, -c(1)]

# scale
happiness.X = data.frame(scale(happiness.X))


str(happiness)
```


```{r, fig.height=9, fig.width=10,out.width='18cm'}
pca_mat = prcomp(happiness.X, center=T, scale=T)

# Score and loadings plot:
autoplot(pca_mat, data = happiness.X, colour='Black',
         loadings = TRUE, loadings.colour = 'red',
         loadings.label = TRUE, loadings.label.size = 5, 
         label=T, label.size=4.5)
```


## a) (3P)

(i)
One obvious observation is the strong correlation between the variables \texttt{Logged.GDP.per.capita}, \texttt{Social.support} and \texttt{Healthy.life.expectancy} as shown by the small angle between these. It appears that these variables go hand-in-hand with each other, e.g. that a country with high levels of social support will most likely also have a good economic outlook in terms of GDP per capita. Conversely, there seems to be a negative correlation between the \texttt{Freedom.to.make.life.choices} and \texttt{Perceptions.of.corruption} as indicated by the almost directly opposite angles of the two. Evidently, living in a corrupt country in which financial assets are required to buy favors will seriously hamper the own ability to pursue certain goals in life. 

(ii) 
Afghanistan can be considered an outlier among the above given countries. It is at the most far right corner of the plot (highest value of PC1) and is not near any of the visible clusters. 

 


## b) (4P)

Here, we're going to find out which variables are important by principal component analysis (PCA) and partial least squares regression (PLSR). 

Note that we can naturally assume the followings:
  
* PCA will find out important variables w.r.t explainability of the dataset of the predictors.
* PLSR can find out important variables w.r.t the response in the model, that is, happiness (= `Ladder.score`).


(i) Make a graphical description of the absolute values of the first principal component (= `PC1`) by PCA. You can use a bar plot, or any other graphical description of your choice (see R-hints below). (1P)

```{r, fig.height=9, fig.width=10,out.width='9cm'}
vals <- abs(data.frame(pca_mat$rotation)$PC1)
barplot(vals, width = 3, names.arg = c(cols[3:8]), cex.names = 0.6)
```



(ii) Fit PLSR on `happiness.XY` with a response of `Ladder.score` (= happiness score) and all the remaining variables in that dataset as predictors. (1P)
```{r}
plsr_model <- plsr(happiness.XY$Ladder.score ~ ., data = happiness.XY, scale = T)
summary(plsr_model)
vals_plsr <- plsr_model$loadings[,c('Comp 1')]
```



(iii) Plot a bar graph of the absolute values of the first principal component for `X` (= predictors of `happiness.XY`) by PLSR. Use the same type of plot as in (i) in order to compare. (1P)
```{r, fig.height=9, fig.width=10,out.width='9cm'}
barplot(abs(vals_plsr), width= 3, cex.names = 0.6)
```

(iv) What are the three most important predictor to predict the happiness score based on the PLSR bar graph from (iii)? (1P)
Based on the barplot in (iii), the three most important predictors to assess the happiness score are: 
\begin{enumerate}
\item \texttt{Logged.GDP.per.capita}
\item \texttt{Social.support}
\item \texttt{Healthy.life.expectancy}
\end{enumerate}


## c) (2P) - Multiple choice

Say for *each* of them if it is true or false.

(i) K-means is optimizing clusters such that the within-cluster variance becomes large.
FALSE
(ii) No matter how many times you run K-means clustering, its cluster centroids will always end up in the same locations.
FALSE
(iii) Strong correlation between predictors allows PCR to be more effective for predicting a response when prediction is made based on the first two principal components.
FALSE
(iv) We can do outlier/anomaly detection with PCA.
TRUE




## d) (3P)

(i)
One easy way to achieve the desired clustering is by setting K = 4. 
```{r, fig.height=10, fig.width=10, eval=T}

K = 4  # your choice
km.out = kmeans(happiness.X, K)

autoplot(pca_mat, data = happiness.X, colour=km.out$cluster,
         label=T, label.size=5,
         loadings = T, loadings.colour = 'blue',
         loadings.label = T, loadings.label.size = 3)
```


(ii)
In a first instance we calculate the mean Ladder scores for each of the clusters, to later interpret them in the light of the previous observations. 
```{r}
# Sort the output according to the cluster
clusters.sort <- sort(km.out$cluster)
# Extract indexes for the clusters
clusters.score <- vector(,K)
Ind <- vector(,K)
Ind[1] = 1
for (i in 1:3){
Ind[i+1] <- min(which(clusters.sort > i))
names <- names(clusters.sort[Ind[i]:Ind[i+1]-1])
clusters.score[i] <- mean(happiness.XY[names,"Ladder.score"])
}
names <- names(clusters.sort[Ind[i+1]:149])
clusters.score[4] <- mean(happiness.XY[names,"Ladder.score"])
print(clusters.score)
```
It becomes apparent that there is a clear hierarchy in the ladder scores for the four produced clusters. The cluster which is centered at low values of both principal components and which includes countries such as Denmark and Norway has the highest ladder score, while the cluster at the far most right of the plot, at high values of PC1, has the lowest ladder score. The second happiest countries are those which are centered near moderately low values of PC1 and moderately high values of PC2, including countries such as South Korea and Japan. Countries which are centered around low values of PC2, such as e.g. Kosovo, are the third happiest in the presented clustering. 

These results can be interpreted when considering the loading directions shown in the above plot. It can be argued based on the mean ladder score of the second happiest cluster together with the loading directions of GDP per capita, social support and healthy life expectancy that "hard factors" (e.g. good personal economic outlook) provide a certain level of minimal satisfaction across countries. However, cultural "soft factors" may also play a significant role, which can be exhibited when analyzing the happiest cluster in the lower left corner of the plot: Clearly, the countries present therein also have very good economic conditions (as indicated by low values of PC1), however, as evident by the low values of PC2 they also provide some combination of generosity and, importantly, the freedom to pursue one's own life choices. The impact of social aspects on the happiness of the poorer countries is not as straightforward to assess, as here the economic outlook seems to be most important. As such, the unhappiest countries exhibit highest values of PC1, and coincidentally among those are some of the poorest countries of the world (e.g. Burundi, Sierra Leone, Madagascar and Afghanistan). The above interpretations can hence be summarized as follows: While improving the economic conditions for poor countries will provide highest improvement in overall life satisfaction, richer countries with a certain minimal level of wealth will most likely respond to improvements in social aspects, such as the freedom to make life choices.
